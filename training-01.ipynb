{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一个例子\n",
    "使用ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2节点+1条边的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes={'__start__': PregelNode(config={'tags': ['langsmith:hidden']}, channels=['__start__'], triggers=['__start__'], writers=[ChannelWrite<__root__>(recurse=True, writes=[ChannelWriteEntry(channel='__root__', value=<object object at 0x7026017ccb60>, skip_none=True, mapper=None)]), ChannelWrite<start:oracle>(recurse=True, writes=[ChannelWriteEntry(channel='start:oracle', value='__start__', skip_none=False, mapper=None)])]), 'oracle': PregelNode(config={'tags': []}, channels=['__root__'], triggers=['start:oracle'], writers=[ChannelWrite<oracle,__root__>(recurse=True, writes=[ChannelWriteEntry(channel='oracle', value='oracle', skip_none=False, mapper=None), ChannelWriteEntry(channel='__root__', value=<object object at 0x7026017ccb60>, skip_none=True, mapper=None)])])} channels={'__root__': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x7026083fad50>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7026004fcce0>, 'oracle': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7026004d5250>, 'start:oracle': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7026003e9a00>} auto_validate=False stream_mode='updates' output_channels='__root__' stream_channels='__root__' input_channels='__start__' builder=<langgraph.graph.message.MessageGraph object at 0x702608d6be60>\n",
      "\n",
      "ret_list\n",
      "content='What is 1+1=?' id='f75c54fd-02e1-429d-883b-dc99945f835d'\n",
      "content='The answer is 2.\\n\\n1 + 1 = 2.' id='f7703ac4-83bd-492c-99b4-813802c5f96b'\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import END, MessageGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "graph = MessageGraph()          #创建一个消息图\n",
    "model=Ollama(model=\"gemma2:2b\", temperature=0.3)\n",
    "graph.add_node(\"oracle\", model) #节点的名字是oracle\n",
    "\n",
    "graph.add_edge(\"oracle\", END)   #添加了一条边的起点就是model，终点就是END\n",
    "#graph.add_conditional_edges\n",
    "graph.set_entry_point(\"oracle\") #定义了起点\n",
    "\n",
    "runnable:CompiledStateGraph = graph.compile()      #编译图，编译后就可以直接运行了\n",
    "print(runnable)\n",
    "ret_list=runnable.invoke(HumanMessage('What is 1+1=?')) #ret_list=runnable.invoke('What is 1+1=?') 这样写也可以\n",
    "print(\"\\nret_list\")\n",
    "for t in ret_list:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 传递参数\n",
    "1. 使用custom_data从头传递到尾\n",
    "2. 无法使用threading.local()实现，因为图的驱动是多线程的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes={'__start__': PregelNode(config={'tags': ['langsmith:hidden']}, channels=['__start__'], triggers=['__start__'], writers=[ChannelWrite<__root__>(recurse=True, writes=[ChannelWriteEntry(channel='__root__', value=<object object at 0x7d9ed0048470>, skip_none=True, mapper=None)]), ChannelWrite<start:node1>(recurse=True, writes=[ChannelWriteEntry(channel='start:node1', value='__start__', skip_none=False, mapper=None)])]), 'node1': PregelNode(config={'tags': []}, channels=['__root__'], triggers=['start:node1'], writers=[ChannelWrite<node1,__root__>(recurse=True, writes=[ChannelWriteEntry(channel='node1', value='node1', skip_none=False, mapper=None), ChannelWriteEntry(channel='__root__', value=<object object at 0x7d9ed0048470>, skip_none=True, mapper=None)])]), 'node2': PregelNode(config={'tags': []}, channels=['__root__'], triggers=['node1'], writers=[ChannelWrite<node2,__root__>(recurse=True, writes=[ChannelWriteEntry(channel='node2', value='node2', skip_none=False, mapper=None), ChannelWriteEntry(channel='__root__', value=<object object at 0x7d9ed0048470>, skip_none=True, mapper=None)])])} channels={'__root__': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x7d9ec9a0c860>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7d9ec9a0c6e0>, 'node1': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7d9ec9bbe9c0>, 'node2': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7d9ec9bf2270>, 'start:node1': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7d9ec9262870>} auto_validate=False stream_mode='updates' output_channels='__root__' stream_channels='__root__' input_channels='__start__' builder=<langgraph.graph.message.MessageGraph object at 0x7d9ec9bf0290>\n",
      "node1: [HumanMessage(content='首发消息', name='first', id='2730e40a-0bce-46a2-84c5-097504374f52', custom_data=100)]\n",
      "node2: [HumanMessage(content='首发消息', name='first', id='2730e40a-0bce-46a2-84c5-097504374f52', custom_data=100), HumanMessage(content='node1 done', name='node1-response', id='714942f7-8600-4b4d-9d35-d4db6b5952ed', custom_data=100)]，custom_data=100\n",
      "\n",
      "ret_list\n",
      "[human] : 首发消息, id=2730e40a-0bce-46a2-84c5-097504374f52\n",
      "[human] : node1 done, id=714942f7-8600-4b4d-9d35-d4db6b5952ed\n",
      "[human] : node2 done, id=88681647-efd0-4337-a607-c28ef1c03be4\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import END, MessageGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.messages import HumanMessage, BaseMessage\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "def node1_process(state: List[BaseMessage]) -> str:\n",
    "    print(f\"node1: {str(state)}\")\n",
    "    ret:HumanMessage=HumanMessage(\"node1 done\")\n",
    "    ret.custom_data=state[-1].custom_data\n",
    "    ret.name=\"node1-response\"\n",
    "    return ret\n",
    "\n",
    "\n",
    "def node2_process(state: List[BaseMessage]) -> str:\n",
    "    custom_data=state[-1].custom_data\n",
    "    print(f\"node2: {str(state)}，custom_data={custom_data}\")\n",
    "    return \"node2 done\"\n",
    "\n",
    "\n",
    "graph = MessageGraph()  # 创建一个消息图\n",
    "\n",
    "model = Ollama(model=\"gemma2:2b\", temperature=0.3)\n",
    "graph.add_node(\"node1\", node1_process)\n",
    "graph.add_node(\"node2\", node2_process)\n",
    "graph.add_edge(\"node1\", \"node2\")\n",
    "graph.add_edge(\"node2\", END)\n",
    "graph.set_entry_point(\"node1\")  # 定义了起点\n",
    "\n",
    "runnable: CompiledStateGraph = graph.compile()  # 编译图，编译后就可以直接运行了\n",
    "print(runnable)\n",
    "message=HumanMessage(\"首发消息\")\n",
    "message.name=\"first\"\n",
    "message.custom_data=100\n",
    "ret_list = runnable.invoke(input=[message])   # 这样写也可以ret_list = runnable.invoke(input='What is 1+1=?')\n",
    "\n",
    "print(\"\\nret_list\")\n",
    "for t in ret_list:\n",
    "    print(f\"[{t.type}] : {t.content}, id={t.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把chain实例作为节点的例子\n",
    "1. 演示了多次大模型调用，后面的大模型请求时需要用到前一次的处理结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open\n",
      "\n",
      "ret_list is\n",
      "content=['打开电脑'] id='e989db82-c837-4afb-99b1-c08f38971a43'\n",
      "content='打开' id='8b9e0703-56fc-4de1-9c55-a8ea12866cae'\n",
      "content='Open' id='01808ec2-8dc0-406f-9f39-15ed8ef1f9f4'\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import List, Union\n",
    "\n",
    "from langgraph.graph import END, MessageGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "#os.environ[\"LANGCHAIN_TRACING\"]=\"true\"  #配置了会向langsmith发送数据\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "def extract_content_func1(inputs: str):\n",
    "    return inputs.strip()\n",
    "\n",
    "graph = MessageGraph()\n",
    "llm_model = Ollama(model=\"gemma2:9b\", temperature=0.3, verbose=True)\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"从语句中提取动词，要求返回中文\"),\n",
    "    MessagesPlaceholder(variable_name=\"name\"),\n",
    "])\n",
    "\n",
    "chain1 = prompt1 | llm_model | extract_content_func1\n",
    "\n",
    "def extract_content_func2(inputs:Union[List[BaseMessage], str]):\n",
    "    if isinstance(inputs, List):\n",
    "        return str(inputs[-1]).strip()\n",
    "    elif isinstance(inputs, str):\n",
    "        return inputs.strip()\n",
    "    else:\n",
    "        raise Exception(f\"extract_content_func2不支持此类型的输入[{type(inputs)}]\")\n",
    "\n",
    "\n",
    "def node2_process(inputs: List[BaseMessage]):\n",
    "    message_list = [SystemMessage(content=\"你是一个专业翻译，现在请你把给你的汉字都翻译成英文\", name=\"system\"),\n",
    "           HumanMessage(content= inputs[-1].content, name=\"human\")\n",
    "          ]\n",
    "    response=llm_model.invoke(message_list)\n",
    "    ret=extract_content_func2(response)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def node3_process(inputs: List[BaseMessage]):\n",
    "    print(inputs[-1].content)\n",
    "\n",
    "graph.add_node(\"node1\", chain1)         #节点的名字是node1，添加的是一个链\n",
    "graph.add_node(\"node2\", node2_process)  #节点node2的作用是接受node1的输出，并继续调用大模型翻译成英文\n",
    "graph.add_node(\"node3\", node3_process)  #打印输出结果\n",
    "graph.add_edge(\"node1\", \"node2\")\n",
    "graph.add_edge(\"node2\", \"node3\")\n",
    "graph.add_edge(\"node3\", END)\n",
    "\n",
    "graph.set_entry_point(\"node1\")\n",
    "\n",
    "runnable: CompiledStateGraph = graph.compile()\n",
    "ret_list = runnable.invoke(HumanMessage([\"打开电脑\"]))\n",
    "print(\"\\nret_list is\")\n",
    "for t in ret_list:\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 添加了条件边的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "执行node2节点\n",
      "\ttype=human,name=human,content=打开电脑,id=54d5b47e-90fb-4dfa-bd0c-3e43250a0abe, additional_kwargs={}\n",
      "\ttype=ai,name=ai,content=开放电脑。,id=6ab9e8df-09ff-4fdd-856d-275011402ec9, additional_kwargs={}\n",
      "\n",
      "ret_list\n",
      "\ttype=human,name=human,content=打开电脑,id=54d5b47e-90fb-4dfa-bd0c-3e43250a0abe, additional_kwargs={}\n",
      "\ttype=ai,name=ai,content=开放电脑。,id=6ab9e8df-09ff-4fdd-856d-275011402ec9, additional_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "from langgraph.graph import END, MessageGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "def print_message(message:BaseMessage)->None:\n",
    "    print(f\"\\ttype={message.type},name={message.name},content={message.content},id={message.id}, additional_kwargs={message.additional_kwargs}\")\n",
    "\n",
    "\n",
    "print()\n",
    "if __name__==\"__main__\":\n",
    "    def after_model_process(val:str)->Union[str, int, float, bool, BaseMessage]:\n",
    "        \"\"\"\n",
    "        llm执行后的处理函数，主要用来从结果中提取数据\n",
    "        \"\"\"\n",
    "        return AIMessage(name=\"ai\", content=val[val.rfind(\"**\")+2:])\n",
    "\n",
    "    graph = MessageGraph()          #创建一个消息图\n",
    "    model=Ollama(model=\"gemma:7b\", temperature=0.3)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"从语句中提取动词，要求返回中文，并且不要带标点符号\"),\n",
    "        MessagesPlaceholder(variable_name=\"name\")])\n",
    "    prompt.name=\"prompt1\"\n",
    "    chain = prompt | model | after_model_process\n",
    "\n",
    "    def node0_process(state:List[BaseMessage]):\n",
    "        \"\"\"\n",
    "        节点0的处理函数\n",
    "        :param state:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        state[-1].data=100\n",
    "        pass\n",
    "\n",
    "    def node1_process(state: List[BaseMessage]):\n",
    "        print(\"node1节点不会被执行\")\n",
    "\n",
    "    def node2_process(state: List[BaseMessage]):\n",
    "        print(\"执行node2节点\")\n",
    "        for item in state: print_message(item)\n",
    "\n",
    "    def edge_route1(state:List[BaseMessage])->str:\n",
    "        \"\"\"\n",
    "        条件边的路由函数，返回的字符串必须和add_condition_edges添加的condition_edge_mapping的key保持一致\n",
    "        :param state:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return \"节点2\"\n",
    "\n",
    "    graph.add_node(\"node0\", node0_process)\n",
    "    graph.add_node(\"oracle\", chain)         #节点的名字是oracle\n",
    "    graph.add_node(\"node1\", node1_process)  #添加了一个node1节点\n",
    "    graph.add_node(\"node2\", node2_process)  # 添加了一个node2节点\n",
    "    graph.add_conditional_edges(\"oracle\", edge_route1,  {\n",
    "        \"节点1\": \"node1\",     #key是名字，值是可选的返回节点名字\n",
    "        \"节点2\": \"node2\"\n",
    "    })   #oracle->node1 | node2\n",
    "    graph.add_edge(\"node0\", \"oracle\")   #node0->oracle\n",
    "    graph.add_edge(\"node1\", END)   #node1->END\n",
    "    graph.add_edge(\"node2\", END)  # node2->END\n",
    "    graph.set_entry_point(\"node0\") #定义了起点\n",
    "\n",
    "    runnable:CompiledGraph = graph.compile()      #编译图，编译后就可以直接运行了\n",
    "    message_list:BaseMessage=HumanMessage(name=\"human\", content=\"打开电脑\")\n",
    "    ret_list=runnable.invoke(message_list)\n",
    "    print(\"\\nret_list\")\n",
    "    for t in ret_list:\n",
    "        print_message(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = '**系统：** 从语句“打开电脑”中提取动词，返回中文，结果为“打开”。'\n",
    "\n",
    "def extract_string(text):\n",
    "    pattern=r'[“\"]([^”\"]*)[”\"]\\s*'\n",
    "    extracted_string = re.findall(pattern, text)\n",
    "    return extracted_string[-1]\n",
    "\n",
    "# 示例\n",
    "extracted_string = extract_string(text)\n",
    "print(extracted_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环执行节点\n",
    "传入一段话提取其中的魔法数字，然后找出1与魔法数字之间的奇数，最终输出奇数的数量\n",
    "1. 大模型调用前后都加了事件方法\n",
    "2. 增加了条件边\n",
    "3. 增加了条件循环\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "执行invoke node节点第1次，当前值:21 是奇数\n",
      "执行invoke node节点第2次，当前值:20\n",
      "执行invoke node节点第3次，当前值:19 是奇数\n",
      "执行invoke node节点第4次，当前值:18\n",
      "执行invoke node节点第5次，当前值:17 是奇数\n",
      "执行invoke node节点第6次，当前值:16\n",
      "执行invoke node节点第7次，当前值:15 是奇数\n",
      "执行invoke node节点第8次，当前值:14\n",
      "执行invoke node节点第9次，当前值:13 是奇数\n",
      "执行invoke node节点第10次，当前值:12\n",
      "执行invoke node节点第11次，当前值:11 是奇数\n",
      "执行invoke node节点第12次，当前值:10\n",
      "执行invoke node节点第13次，当前值:9 是奇数\n",
      "执行invoke node节点第14次，当前值:8\n",
      "执行invoke node节点第15次，当前值:7 是奇数\n",
      "执行invoke node节点第16次，当前值:6\n",
      "执行invoke node节点第17次，当前值:5 是奇数\n",
      "执行invoke node节点第18次，当前值:4\n",
      "执行invoke node节点第19次，当前值:3 是奇数\n",
      "执行invoke node节点第20次，当前值:2\n",
      "执行invoke node节点第21次，当前值:1 是奇数\n",
      "结论：总共执行了22次，奇数有11个\n",
      "\n",
      "执行结束，返回结果\n",
      "\ttype=human,name=human,content=我给出的魔法数字是21，这个数字有魔力的哦,id=7c8cfa5c-f9e7-46b5-af43-bbb2a29da584, additional_kwargs={'loop_count': 22}\n",
      "\ttype=ai,name=ai,content=答案：21\n",
      "\n",
      "魔法数字是21，根据人说的。,id=e7ebb7ea-765d-4022-8fa4-93f72b0d9777, additional_kwargs={'value': 0, 'odd_count': 11}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union, Dict, Any, Optional\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "from langgraph.graph import END, MessageGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import random\n",
    "import sys\n",
    "\n",
    "def print_message(message:BaseMessage)->None:\n",
    "    print(f\"\\ttype={message.type},name={message.name},content={message.content},id={message.id}, additional_kwargs={message.additional_kwargs}\")\n",
    "\n",
    "print()\n",
    "PREFIX=\"答案：\"\n",
    "if __name__==\"__main__\":\n",
    "    sys.setrecursionlimit(5000) #设置全局递归上限，太小了执行会报错\n",
    "\n",
    "    def before_oracle(val:ChatPromptValue)->ChatPromptValue:\n",
    "        \"\"\"\n",
    "        llm执行前的事件\n",
    "        \"\"\"\n",
    "        return val\n",
    "\n",
    "    def after_oracle(val:str)->Union[str, int, float, bool, BaseMessage]:\n",
    "        \"\"\"\n",
    "        llm执行后的处理函数，主要用来从结果中提取数据\n",
    "        \"\"\"\n",
    "        temp=val[len(PREFIX):val.rfind(\"\\n\\n\")]\n",
    "        ret:AIMessage=AIMessage(name=\"ai\", content=val)\n",
    "        ret.additional_kwargs['value']=int(temp)\n",
    "        ret.additional_kwargs['odd_count']=0\n",
    "        return ret\n",
    "\n",
    "    graph = MessageGraph()          #创建一个消息图\n",
    "    model=Ollama(model=\"gemma:7b\", temperature=0.3)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"\"\"\n",
    "##目标\n",
    "请帮我提取这个魔法数字，回答保持极简\n",
    "\n",
    "##输出\n",
    "{PREFIX}Number\n",
    "\n",
    "##注意事项\n",
    "只能输出魔法数字，不要出现别的文字\n",
    "         \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"name\")])\n",
    "    prompt.name=\"prompt1\"\n",
    "    chain = prompt | before_oracle | model | after_oracle\n",
    "\n",
    "    def init_process(state:List[BaseMessage]):\n",
    "        \"\"\"\n",
    "        节点初始化处理函数\n",
    "        \"\"\"\n",
    "        humanMessage:HumanMessage=state[0] # type: ignore\n",
    "        assert isinstance(humanMessage, HumanMessage)\n",
    "        humanMessage.additional_kwargs['loop_count']=1\n",
    "\n",
    "    def node_invoke_process(state:List[BaseMessage]):\n",
    "        \"\"\"\n",
    "        执行节点，负责执奇数判定和计数器\n",
    "        \"\"\"\n",
    "        humanMessage:HumanMessage=state[0] # type: ignore\n",
    "        assert isinstance(humanMessage, HumanMessage)\n",
    "\n",
    "        aiMessage:AIMessage=state[1] # type: ignore\n",
    "        assert isinstance(aiMessage, AIMessage)\n",
    "        is_odd:bool=aiMessage.additional_kwargs['value'] % 2==1\n",
    "        if is_odd:\n",
    "            aiMessage.additional_kwargs['odd_count']+=1\n",
    "        print(f\"执行invoke node节点第{humanMessage.additional_kwargs['loop_count']}次，当前值:{aiMessage.additional_kwargs['value']}\"+(\" 是奇数\" if is_odd else \"\"))\n",
    "        aiMessage.additional_kwargs['value']-=1\n",
    "        humanMessage.additional_kwargs['loop_count']+=1\n",
    "\n",
    "    def edge_invoke_route(state:List[BaseMessage])->str:\n",
    "        \"\"\"\n",
    "        校验边，负责决定结束条件，如果不满足条件就退回\n",
    "        \"\"\"\n",
    "        aiMessage:AIMessage=state[1] # type: ignore\n",
    "        assert isinstance(aiMessage, AIMessage)\n",
    "        if aiMessage.additional_kwargs['value']==0:\n",
    "            return \"输出结论\"\n",
    "        return \"节点invoke\"\n",
    "\n",
    "    def node_conclusion_process(state: List[BaseMessage]):\n",
    "        \"\"\"\n",
    "        节点-输出结论\n",
    "        \"\"\"\n",
    "        humanMessage:HumanMessage=state[0] # type: ignore\n",
    "        assert isinstance(humanMessage, HumanMessage)\n",
    "\n",
    "        aiMessage:AIMessage=state[1] # type: ignore\n",
    "        assert isinstance(aiMessage, AIMessage)\n",
    "\n",
    "        print(f\"结论：总共执行了{humanMessage.additional_kwargs['loop_count']}次，奇数有{aiMessage.additional_kwargs['odd_count']}个\")\n",
    "\n",
    "    graph.add_node(\"node_init\", init_process)                   #添加一个初始化节点\n",
    "    graph.add_node(\"oracle\", chain)\n",
    "    graph.add_node(\"node_invoke\", node_invoke_process)          #添加一个执行节点\n",
    "    graph.add_node(\"node_conclusion\", node_conclusion_process)  #添加了一个结论节点\n",
    "\n",
    "    graph.add_conditional_edges(\"node_invoke\", edge_invoke_route,  {\n",
    "        \"节点invoke\": \"node_invoke\",        #key是名字，值是可选的返回节点名字\n",
    "        \"输出结论\": \"node_conclusion\"\n",
    "    })                                      #条件边：初始化节点可以到执行节点也可以到结论节点\n",
    "    graph.add_edge(\"node_init\", \"oracle\")   #初始化节点到大模型节点\n",
    "    graph.add_edge(\"oracle\", \"node_invoke\") #大模型节点到执行节点\n",
    "    graph.add_edge(\"node_conclusion\", END)  #结论节点到结束节点\n",
    "    graph.set_entry_point(\"node_init\")      #定义了起点\n",
    "\n",
    "    runnable:CompiledGraph = graph.compile()      #编译图，编译后就可以直接运行了\n",
    "    message_list:BaseMessage=HumanMessage(name=\"human\", content=f\"我给出的魔法数字是{random.randint(5,100)}，这个数字有魔力的哦\")\n",
    "    ret_list:List[BaseMessage]=runnable.invoke(message_list, config={\n",
    "        \"recursion_limit\": 3000,    #设置LangChain的递归上限值，太小了会抛出GraphRecursionError异常\n",
    "    }) #type: ignore\n",
    "    print(\"\\n执行结束，返回结果\")\n",
    "    for t in ret_list:\n",
    "        print_message(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用StateGraph的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "执行invoke node节点第1次，当前值:28\n",
      "执行invoke node节点第2次，当前值:27 是奇数\n",
      "执行invoke node节点第3次，当前值:26\n",
      "执行invoke node节点第4次，当前值:25 是奇数\n",
      "执行invoke node节点第5次，当前值:24\n",
      "执行invoke node节点第6次，当前值:23 是奇数\n",
      "执行invoke node节点第7次，当前值:22\n",
      "执行invoke node节点第8次，当前值:21 是奇数\n",
      "执行invoke node节点第9次，当前值:20\n",
      "执行invoke node节点第10次，当前值:19 是奇数\n",
      "执行invoke node节点第11次，当前值:18\n",
      "执行invoke node节点第12次，当前值:17 是奇数\n",
      "执行invoke node节点第13次，当前值:16\n",
      "执行invoke node节点第14次，当前值:15 是奇数\n",
      "执行invoke node节点第15次，当前值:14\n",
      "执行invoke node节点第16次，当前值:13 是奇数\n",
      "执行invoke node节点第17次，当前值:12\n",
      "执行invoke node节点第18次，当前值:11 是奇数\n",
      "执行invoke node节点第19次，当前值:10\n",
      "执行invoke node节点第20次，当前值:9 是奇数\n",
      "执行invoke node节点第21次，当前值:8\n",
      "执行invoke node节点第22次，当前值:7 是奇数\n",
      "执行invoke node节点第23次，当前值:6\n",
      "执行invoke node节点第24次，当前值:5 是奇数\n",
      "执行invoke node节点第25次，当前值:4\n",
      "执行invoke node节点第26次，当前值:3 是奇数\n",
      "执行invoke node节点第27次，当前值:2\n",
      "执行invoke node节点第28次，当前值:1 是奇数\n",
      "结论：总共执行了29次，奇数有14个\n",
      "\n",
      "执行结束，返回结果\n",
      "\ttype=human,name=human,content=我给出的魔法数字是28，这个数字有魔力的哦,id=1059f3c2-801c-405b-b6fb-c893e260f1f5, additional_kwargs={'loop_count': 29}\n",
      "\ttype=system,name=system,content=你是一个数学家，擅长处理各种数字,id=c8bb0100-6f69-4677-a122-5f9acbb28caa, additional_kwargs={}\n",
      "\ttype=ai,name=ai,content=答案：28 \n",
      ",id=fbdba53c-730d-4876-b466-c620b3959406, additional_kwargs={'value': 0, 'odd_count': 14}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, TypedDict, Annotated, Sequence, Union, Dict, Any, Optional\n",
    "import operator\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "from langgraph.graph import END, StateGraph, MessageGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "def print_message(message: BaseMessage) -> None:\n",
    "    print(f\"\\ttype={message.type},name={message.name},content={message.content},id={message.id}, additional_kwargs={message.additional_kwargs}\")\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    定义Agent状态类，其中重载了+操作符，实现了通过+来向AgentState里添加消息\n",
    "    \"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "\n",
    "print()\n",
    "PREFIX = \"答案：\"\n",
    "if __name__ == \"__main__\":\n",
    "    sys.setrecursionlimit(5000)  # 设置全局递归上限，太小了执行会报错\n",
    "\n",
    "\n",
    "    def before_oracle(val: ChatPromptValue) -> ChatPromptValue:\n",
    "        \"\"\"\n",
    "        llm执行前的事件\n",
    "        \"\"\"\n",
    "        return val\n",
    "\n",
    "\n",
    "    def after_oracle(val: str) -> Union[str, int, float, bool, BaseMessage]:\n",
    "        \"\"\"\n",
    "        llm执行后的处理函数，主要用来从结果中提取数据\n",
    "        \"\"\"\n",
    "        temp = val[len(PREFIX):val.rfind(\"\\n\\n\")]\n",
    "        ret: AIMessage = AIMessage(name=\"ai\", content=val)\n",
    "        ret.additional_kwargs['value'] = int(temp)\n",
    "        ret.additional_kwargs['odd_count'] = 0\n",
    "        return ret\n",
    "\n",
    "    graph = MessageGraph()  # 创建一个状态图\n",
    "    model = Ollama(model=\"gemma2:2b\", temperature=0.3)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"\"\"\n",
    "##目标\n",
    "请帮我提取这个魔法数字，回答保持极简\n",
    "\n",
    "##输出\n",
    "{PREFIX}\n",
    "\n",
    "##注意事项\n",
    "只能输出魔法数字，不要出现别的文字\n",
    "         \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"name\")])\n",
    "    prompt.name = \"prompt1\"\n",
    "    chain = prompt | before_oracle | model | after_oracle\n",
    "\n",
    "\n",
    "    def init_process(state: List[BaseMessage]):\n",
    "        \"\"\"\n",
    "        节点初始化处理函数\n",
    "        \"\"\"\n",
    "        humanMessage: HumanMessage = state[0]  # type: ignore\n",
    "        assert isinstance(humanMessage, HumanMessage)\n",
    "        humanMessage.additional_kwargs['loop_count'] = 1\n",
    "\n",
    "\n",
    "    def node_invoke_process(state: List[BaseMessage]):\n",
    "        \"\"\"\n",
    "        执行节点，负责执奇数判定和计数器\n",
    "        \"\"\"\n",
    "        humanMessage: HumanMessage = [item for item in state if item.type=='human'][0]  # type: ignore\n",
    "        assert isinstance(humanMessage, HumanMessage)\n",
    "\n",
    "        aiMessage: AIMessage = [item for item in state if item.type=='ai'][0]  # type: ignore\n",
    "        assert isinstance(aiMessage, AIMessage)\n",
    "        is_odd: bool = aiMessage.additional_kwargs['value'] % 2 == 1\n",
    "        if is_odd:\n",
    "            aiMessage.additional_kwargs['odd_count'] += 1\n",
    "        print(f\"执行invoke node节点第{humanMessage.additional_kwargs['loop_count']}次，当前值:{aiMessage.additional_kwargs['value']}\" + (\" 是奇数\" if is_odd else \"\"))\n",
    "        aiMessage.additional_kwargs['value'] -= 1\n",
    "        humanMessage.additional_kwargs['loop_count'] += 1\n",
    "\n",
    "\n",
    "    def edge_invoke_route(state: List[BaseMessage]) -> str:\n",
    "        \"\"\"\n",
    "        校验边，负责决定结束条件，如果不满足条件就退回\n",
    "        \"\"\"\n",
    "        aiMessage: AIMessage = [item for item in state if item.type=='ai'][0]  # type: ignore\n",
    "        assert isinstance(aiMessage, AIMessage)\n",
    "        if aiMessage.additional_kwargs['value'] == 0:\n",
    "            return \"输出结论\"\n",
    "        return \"节点invoke\"\n",
    "\n",
    "\n",
    "    def node_conclusion_process(state: List[BaseMessage]):\n",
    "        \"\"\"\n",
    "        节点-输出结论\n",
    "        \"\"\"\n",
    "        humanMessage: HumanMessage = [item for item in state if item.type=='human'][0]  # type: ignore\n",
    "        assert isinstance(humanMessage, HumanMessage)\n",
    "\n",
    "        aiMessage: AIMessage = [item for item in state if item.type=='ai'][0]  # type: ignore\n",
    "        assert isinstance(aiMessage, AIMessage)\n",
    "\n",
    "        print(f\"结论：总共执行了{humanMessage.additional_kwargs['loop_count']}次，奇数有{aiMessage.additional_kwargs['odd_count']}个\")\n",
    "\n",
    "\n",
    "    graph.add_node(\"node_init\", init_process)  # 添加一个初始化节点\n",
    "    graph.add_node(\"oracle\", chain)\n",
    "    graph.add_node(\"node_invoke\", node_invoke_process)  # 添加一个执行节点\n",
    "    graph.add_node(\"node_conclusion\", node_conclusion_process)  # 添加了一个结论节点\n",
    "\n",
    "    graph.add_conditional_edges(\"node_invoke\", edge_invoke_route, {\n",
    "        \"节点invoke\": \"node_invoke\",  # key是名字，值是可选的返回节点名字\n",
    "        \"输出结论\": \"node_conclusion\"\n",
    "    })  # 条件边：初始化节点可以到执行节点也可以到结论节点\n",
    "    graph.add_edge(\"node_init\", \"oracle\")  # 初始化节点到大模型节点\n",
    "    graph.add_edge(\"oracle\", \"node_invoke\")  # 大模型节点到执行节点\n",
    "    graph.add_edge(\"node_conclusion\", END)  # 结论节点到结束节点\n",
    "    graph.set_entry_point(\"node_init\")  # 定义了起点\n",
    "\n",
    "    runnable: CompiledGraph = graph.compile()  # 编译图，编译后就可以直接运行了\n",
    "    # message_list:BaseMessage=HumanMessage(name=\"human\", content=f\"我给出的魔法数字是{random.randint(5,100)}，这个数字有魔力的哦\")\n",
    "\n",
    "    message_list: List[BaseMessage] = [\n",
    "        HumanMessage(name=\"human\", content=f\"我给出的魔法数字是{random.randint(5, 100)}，这个数字有魔力的哦\"),\n",
    "        SystemMessage(name=\"system\", content=f\"你是一个数学家，擅长处理各种数字\")\n",
    "    ]\n",
    "\n",
    "    ret_list: List[BaseMessage] = runnable.invoke(message_list, config={\n",
    "        \"recursion_limit\": 3000,  # 设置LangChain的递归上限值，太小了会抛出GraphRecursionError异常\n",
    "    })  # type: ignore\n",
    "    print(\"\\n执行结束，返回结果\")\n",
    "    for t in ret_list:\n",
    "        print_message(t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
